# RAGサンプルプロジェクト

LangChainを使用したRAG（Retrieval-Augmented Generation）のサンプル実装です。

## 機能

1. **シンプルRAG**: 文書検索のみ（LLMなし）
2. **完全RAG**: Ollamaを使用した回答生成機能付き
3. **カスタムプロンプトテンプレート**: 3種類のプロンプトから選択可能

### プロンプトテンプレートの種類

- **basic**: 基本的なプロンプト（シンプル）
- **detailed**: 詳細なルール付きプロンプト（推奨）
- **technical**: 技術文書特化プロンプト（技術的な質問に最適）

## セットアップ

### 1. 依存関係のインストール
```bash
uv sync
```

### 2. Ollama設定（完全RAG機能を使用する場合）

1. Ollamaをインストール:
   ```bash
   # Linux/macOS
   curl -fsSL https://ollama.com/install.sh | sh
   
   # または公式サイトからダウンロード: https://ollama.com/
   ```

2. モデルをダウンロード:
   ```bash
   ollama pull mistral
   # または軽量版: ollama pull llama3.2:1b
   ```

3. Ollamaサーバーを起動:
   ```bash
   ollama serve
   ```

## 実行方法

```bash
python main.py
```

実行時にモードを選択できます：
- `1`: シンプルデモ（検索のみ）
- `2`: 完全RAGデモ（Ollama + 回答生成）
- `3`: プロンプトテンプレートプレビュー

完全RAGデモでは、さらにプロンプトテンプレートを選択できます：
- `1`: basic - 基本的なプロンプト
- `2`: detailed - 詳細なルール付きプロンプト（推奨）
- `3`: technical - 技術文書特化プロンプト

## プロンプトエンジニアリング機能

### 外部ファイル管理
- プロンプトテンプレートは`prompt_templates.txt`で管理
- ファイル編集により簡単にカスタマイズ可能
- 3種類のテンプレートを自動読み込み

### テンプレート形式
```
## [BASIC]
基本的なプロンプト内容...

## [DETAILED]  
詳細なプロンプト内容...

## [TECHNICAL]
技術特化プロンプト内容...
```

### プロンプトの特徴
- **正確性の確保**: 文書の内容のみに基づく回答
- **幻覚の防止**: 推測や創作を禁止
- **構造化された回答**: 一貫した形式での情報提供
- **初心者配慮**: 技術的内容をわかりやすく説明

## 使用技術

- **LangChain**: RAGフレームワーク
- **Chroma**: ベクトルデータベース
- **HuggingFace**: 埋め込みモデル
- **Ollama**: ローカルLLM実行環境
- **カスタムプロンプトテンプレート**: 回答品質向上

## ファイル構成

- `main.py`: メインプログラム
- `sample_documents.txt`: サンプル文書
- `prompt_templates.txt`: 基本プロンプトテンプレート
- `prompt_templates_advanced.txt`: 高度なプロンプトテンプレート
- `.env`: 環境変数設定
- `chroma_db/`: ベクトルデータベース（自動生成）

## プロンプトテンプレートのカスタマイズ

`load_prompt_template()`関数内のテンプレートを編集することで、独自のプロンプトを作成できます。

### カスタマイズのポイント
1. **システムプロンプト**: LLMの役割と行動規範を明確に定義
2. **入力変数**: `{context}`と`{question}`を適切に配置
3. **出力形式**: 一貫した回答構造を指定
4. **制約条件**: 幻覚防止や正確性確保のルール