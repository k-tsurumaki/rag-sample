import os
import re
import warnings
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

import langextract as lx
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores.utils import filter_complex_metadata
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langextract import inference

# è­¦å‘Šã‚’ç„¡è¦–
warnings.filterwarnings("ignore")


@dataclass
class ExtractedData:
    """æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®è©³ç´°æƒ…å ±"""

    value: Any
    exact_source: str
    page_number: Optional[int] = None
    line_number: Optional[int] = None
    confidence_score: float = 1.0
    cross_references: List[str] = None
    calculation_source: Optional[str] = None

    def __post_init__(self):
        if self.cross_references is None:
            self.cross_references = []


@dataclass
class VerificationData:
    """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿"""

    source_components: Dict[str, str]
    calculated_value: Any
    verification_notes: str


class LangExtractRAG:
    """LangExtractã‚’çµ±åˆã—ãŸæ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.vectorstore = None
        self.qa_chain = None
        self.documents = []
        self.langextract_examples = self._create_langextract_examples()

    def _create_langextract_examples(self):
        """LangExtractç”¨ã®æŠ½å‡ºä¾‹ã‚’ä½œæˆ"""
        examples = [
            lx.data.ExampleData(
                text="Pythonã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚ã‚‹scikit-learnã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®85%ã§ä½¿ç”¨ã•ã‚Œã¦ãŠã‚Šã€2007å¹´ã«æœ€åˆã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸã€‚",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="library_name",
                        extraction_text="scikit-learn",
                        attributes={"category": "æ©Ÿæ¢°å­¦ç¿’", "language": "Python"},
                    ),
                    lx.data.Extraction(
                        extraction_class="usage_statistics",
                        extraction_text="85%",
                        attributes={
                            "metric": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ©ç”¨ç‡",
                            "context": "scikit-learnä½¿ç”¨ç‡",
                        },
                    ),
                    lx.data.Extraction(
                        extraction_class="release_year",
                        extraction_text="2007å¹´",
                        attributes={
                            "event": "æœ€åˆã®ãƒªãƒªãƒ¼ã‚¹",
                            "software": "scikit-learn",
                        },
                    ),
                ],
            ),
            lx.data.ExampleData(
                text="TensorFlowã¯GoogleãŒé–‹ç™ºã—ãŸãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€ç”£æ¥­ç•Œã§åºƒãæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
                extractions=[
                    lx.data.Extraction(
                        extraction_class="framework_name",
                        extraction_text="TensorFlow",
                        attributes={
                            "category": "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°",
                            "developer": "Google",
                        },
                    ),
                    lx.data.Extraction(
                        extraction_class="adoption_info",
                        extraction_text="ç”£æ¥­ç•Œã§åºƒãæ¡ç”¨",
                        attributes={"scope": "ç”£æ¥­ç•Œ", "level": "åºƒç¯„å›²"},
                    ),
                ],
            ),
        ]
        return examples

    def load_documents(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
        with open("sample_documents.txt", "r", encoding="utf-8") as f:
            content = f.read()

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²ã—ã€è¡Œç•ªå·æƒ…å ±ã‚’ä¿æŒ
        sections = content.split("\n\n")
        documents = []
        line_offset = 0

        for i, section in enumerate(sections):
            if section.strip():
                # å„è¡Œã«è¡Œç•ªå·ã‚’ä»˜ä¸
                lines = section.strip().split("\n")
                line_numbers = list(
                    range(line_offset + 1, line_offset + len(lines) + 1)
                )

                doc = Document(
                    page_content=section.strip(),
                    metadata={
                        "source": "sample_documents.txt",
                        "section": i,
                        "start_line": line_offset + 1,
                        "end_line": line_offset + len(lines),
                        "line_numbers": ",".join(
                            map(str, line_numbers)
                        ),  # ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                    },
                )
                documents.append(doc)
                line_offset += len(lines) + 2  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ç©ºè¡Œã‚’è€ƒæ…®

        self.documents = documents
        return documents

    def create_vector_store(self, documents):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ"""
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆã‚ˆã‚Šç´°ã‹ãåˆ†å‰²ã—ã¦è©³ç´°ãªä½ç½®æƒ…å ±ã‚’ä¿æŒï¼‰
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,  # ã‚ˆã‚Šå°ã•ãªãƒãƒ£ãƒ³ã‚¯ã§è©³ç´°ãªä½ç½®æƒ…å ±ã‚’ä¿æŒ
            chunk_overlap=50,
            length_function=len,
        )

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²ã—ã€å„ãƒãƒ£ãƒ³ã‚¯ã«è©³ç´°ãªä½ç½®æƒ…å ±ã‚’ä»˜ä¸
        splits = []
        for doc in documents:
            doc_splits = text_splitter.split_documents([doc])
            for i, split in enumerate(doc_splits):
                # ãƒãƒ£ãƒ³ã‚¯å†…ã®è¡Œç•ªå·ã‚’è¨ˆç®—
                split.metadata.update(
                    {
                        "chunk_id": f"{doc.metadata['section']}_{i}",
                        "chunk_start_char": split.page_content.find(
                            split.page_content.split("\n")[0]
                        )
                        if "\n" in split.page_content
                        else 0,
                    }
                )
                splits.append(split)

        # è¤‡é›‘ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆChromaDBå¯¾å¿œï¼‰
        filtered_splits = filter_complex_metadata(splits)

        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"},
        )

        # Chromaãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        persist_directory = os.getenv(
            "CHROMA_PERSIST_DIRECTORY", "./chroma_db_enhanced"
        )
        vectorstore = Chroma.from_documents(
            documents=filtered_splits,
            embedding=embeddings,
            persist_directory=persist_directory,
        )

        self.vectorstore = vectorstore
        return vectorstore

    def load_enhanced_prompt_template(self):
        """è©³ç´°ãªæŠ½å‡ºç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ"""
        template = """
ã‚ãªãŸã¯è©³ç´°ãªãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€æ­£ç¢ºã§æ¤œè¨¼å¯èƒ½ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

1. æä¾›ã•ã‚ŒãŸæ–‡æ›¸ã‹ã‚‰å…·ä½“çš„ãªæ•°å€¤ã€äº‹å®Ÿã€ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹
2. å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã«ã¤ã„ã¦ã€æ­£ç¢ºãªã‚½ãƒ¼ã‚¹ä½ç½®ï¼ˆè¡Œç•ªå·ã€æ–‡ç« ï¼‰ã‚’ç‰¹å®šã™ã‚‹
3. å¯èƒ½ãªå ´åˆã€ä»–ã®æ–‡æ›¸éƒ¨åˆ†ã§ã®è¨€åŠã‚„é–¢é€£æƒ…å ±ã‚‚ç‰¹å®šã™ã‚‹
4. è¨ˆç®—ãŒå¿…è¦ãªå ´åˆã€ãã®è¨ˆç®—éç¨‹ã¨æ ¹æ‹ ã‚’æ˜ç¤ºã™ã‚‹
5. ä¿¡é ¼åº¦ã‚„ç¢ºå®Ÿæ€§ã«ã¤ã„ã¦ã‚‚è¨€åŠã™ã‚‹

å‚è€ƒæ–‡æ›¸:
{context}

è³ªå•: {question}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

ã€æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã€‘
- ãƒ‡ãƒ¼ã‚¿é …ç›®1: å€¤
  - æ­£ç¢ºãªã‚½ãƒ¼ã‚¹: "æ–‡æ›¸å†…ã®æ­£ç¢ºãªå¼•ç”¨"
  - ä½ç½®æƒ…å ±: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç•ªå·ã€æ¨å®šè¡Œä½ç½®
  - ä¿¡é ¼åº¦: é«˜/ä¸­/ä½
  - é–¢é€£å‚ç…§: ä»–ã®é–¢é€£ç®‡æ‰€

ã€æ¤œè¨¼æƒ…å ±ã€‘
- è¨ˆç®—éç¨‹ã‚„æ¤œè¨¼æ–¹æ³•
- çŸ›ç›¾ã‚„ä¸ç¢ºå®Ÿæ€§ãŒã‚ã‚‹å ´åˆã®æŒ‡æ‘˜

ã€ç·åˆå›ç­”ã€‘
è³ªå•ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªå›ç­”
"""

        return PromptTemplate(
            template=template, input_variables=["context", "question"]
        )

    def extract_with_langextract(self, query: str, context_text: str) -> Dict[str, Any]:
        """LangExtractã‚’ä½¿ç”¨ã—ãŸè©³ç´°ãªæƒ…å ±æŠ½å‡ºï¼ˆå¿…é ˆï¼‰"""
        # LangExtractã‚’ç›´æ¥ä½¿ç”¨ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ãªã„ï¼‰
        return self._extract_with_langextract_ollama(query, context_text)

    def _extract_with_langextract_ollama(
        self, query: str, context_text: str
    ) -> Dict[str, Any]:
        """LangExtractã®Ollamaçµ±åˆç‰ˆï¼ˆå¿…é ˆä½¿ç”¨ï¼‰"""
        # æŠ½å‡ºã‚¿ã‚¹ã‚¯ã®å®šç¾©
        prompt_description = f"""
        ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€è³ªå•ã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æ§‹é€ åŒ–ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
        
        1. æŠ€è¡“åãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å
        2. æ•°å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã€å¹´ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€çµ±è¨ˆãªã©ï¼‰
        3. ç‰¹å¾´ãƒ»æ€§èƒ½ãƒ»åˆ©ç”¨çŠ¶æ³ã®è¨˜è¿°
        4. é–‹ç™ºè€…ãƒ»çµ„ç¹”æƒ…å ±
        5. ãƒªãƒªãƒ¼ã‚¹æ—¥ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
        
        æ­£ç¢ºãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€æ¨æ¸¬ã‚„æ„è¨³ã¯è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚
        å„æŠ½å‡ºé …ç›®ã«å¯¾ã—ã¦ã€æ„å‘³ã®ã‚ã‚‹å±æ€§ã‚’è¿½åŠ ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        """

        # Ollamaè¨­å®šã§LangExtractã‚’å®Ÿè¡Œ
        ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        ollama_model = os.getenv("OLLAMA_MODEL", "mistral")

        # LangExtractã‚’å¿…é ˆã¨ã—ã¦å®Ÿè¡Œ
        result = lx.extract(
            text_or_documents=context_text,
            prompt_description=prompt_description,
            examples=self.langextract_examples,
            extraction_passes=int(os.getenv("LANGEXTRACT_EXTRACTION_PASSES", "2")),
            max_workers=int(os.getenv("LANGEXTRACT_MAX_WORKERS", "3")),
            max_char_buffer=int(os.getenv("LANGEXTRACT_MAX_CHAR_BUFFER", "600")),
            language_model_type=inference.OllamaLanguageModel,
            language_model_params={
                "model": "tinyllama",
                "model_url": "http://localhost:11434",
            },
        )

        # result = lx.extract(
        #     text_or_documents=input_text,
        #     prompt_description=prompt,
        #     examples=examples,
        #     language_model_type=inference.OllamaLanguageModel,
        #     language_model_params={
        #         "model": "tinyllama",
        #         "model_url": "http://localhost:11434",
        #     },
        # )

        # çµæœã‚’æ§‹é€ åŒ–
        structured_result = {
            "extractions": [],
            "extraction_metadata": {
                "model_used": f"ollama/{ollama_model}",
                "api_base": ollama_base_url,
                "extraction_passes": int(
                    os.getenv("LANGEXTRACT_EXTRACTION_PASSES", "2")
                ),
                "source_text_length": len(context_text),
                "total_extractions": len(result.extractions),
                "method": "langextract_ollama",
            },
        }

        # æŠ½å‡ºçµæœã‚’å‡¦ç†
        for extraction in result.extractions:
            extraction_data = {
                "class": extraction.extraction_class,
                "text": extraction.extraction_text,
                "attributes": extraction.attributes
                if hasattr(extraction, "attributes")
                else {},
                "source_position": {
                    "start": getattr(extraction, "start_char", None),
                    "end": getattr(extraction, "end_char", None),
                },
                "confidence": getattr(extraction, "confidence", 1.0),
            }
            structured_result["extractions"].append(extraction_data)

        return structured_result

    def create_qa_chain(self):
        """æ‹¡å¼µQAãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ"""
        if not self.vectorstore:
            raise ValueError("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # æ‹¡å¼µãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        prompt_template = self.load_enhanced_prompt_template()

        # Ollamaãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        llm = OllamaLLM(
            model="mistral",
            base_url="http://localhost:11434",
            temperature=0.1,  # ã‚ˆã‚Šç¢ºå®šçš„ãªå›ç­”ã®ãŸã‚ä½ãè¨­å®š
            num_predict=1024,  # ã‚ˆã‚Šè©³ç´°ãªå›ç­”ã®ãŸã‚é•·ãè¨­å®š
        )

        print("æ‹¡å¼µRAGç”¨Ollamaãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

        # RetrievalQAãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 5}  # ã‚ˆã‚Šå¤šãã®æ–‡æ›¸ã‚’æ¤œç´¢
            ),
            return_source_documents=True,
            verbose=True,
            chain_type_kwargs={
                "prompt": prompt_template,
                "document_variable_name": "context",
            },
        )

        self.qa_chain = qa_chain
        return qa_chain

    def extract_detailed_data(self, query: str) -> Dict[str, Any]:
        """LangExtractã‚’çµ±åˆã—ãŸè©³ç´°ãªãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’å®Ÿè¡Œ"""
        if not self.qa_chain:
            raise ValueError("QAãƒã‚§ãƒ¼ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # åŸºæœ¬çš„ãªQAå®Ÿè¡Œ
        result = self.qa_chain.invoke({"query": query})

        # å…¨ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã‚’çµåˆã—ã¦LangExtractã§æŠ½å‡º
        combined_context = "\n\n".join(
            [doc.page_content for doc in result["source_documents"]]
        )

        # LangExtractã§è©³ç´°æŠ½å‡ºï¼ˆå¿…é ˆï¼‰
        langextract_results = self.extract_with_langextract(query, combined_context)

        # ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã‹ã‚‰è©³ç´°ãªä½ç½®æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¾“æ¥ã®æ–¹æ³•ã‚‚ä½µç”¨ï¼‰
        detailed_sources = self._analyze_source_documents(
            result["source_documents"], query
        )

        # LangExtractã®çµæœã‚’çµ±åˆ
        for i, source in enumerate(detailed_sources):
            # å„ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã«å¯¾ã—ã¦ã‚‚LangExtractæŠ½å‡ºã‚’å®Ÿè¡Œï¼ˆå¿…é ˆï¼‰
            source_langextract = self.extract_with_langextract(
                query, source["content_preview"]
            )
            source["langextract_data"] = source_langextract

        # æ§‹é€ åŒ–ã•ã‚ŒãŸçµæœã‚’ä½œæˆ
        enhanced_result = {
            "query": query,
            "answer": result["result"],
            "langextract_analysis": langextract_results,
            "detailed_sources": detailed_sources,
            "verification_data": self._create_verification_data(
                result["source_documents"], result["result"]
            ),
            "confidence_analysis": self._analyze_confidence(
                result["source_documents"], result["result"]
            ),
            "extraction_summary": self._create_extraction_summary(langextract_results),
        }

        return enhanced_result

    def _create_extraction_summary(self, langextract_results: Dict) -> Dict:
        """LangExtractçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
        extractions = langextract_results.get("extractions", [])

        summary = {
            "total_extractions": len(extractions),
            "extraction_types": {},
            "key_findings": [],
            "numerical_data": [],
            "technical_terms": [],
        }

        for extraction in extractions:
            extraction_class = extraction.get("class", "unknown")

            # æŠ½å‡ºã‚¿ã‚¤ãƒ—ã®çµ±è¨ˆ
            if extraction_class not in summary["extraction_types"]:
                summary["extraction_types"][extraction_class] = 0
            summary["extraction_types"][extraction_class] += 1

            # é‡è¦ãªç™ºè¦‹ã‚’åˆ†é¡
            text = extraction.get("text", "")
            attributes = extraction.get("attributes", {})

            if any(
                keyword in extraction_class.lower()
                for keyword in ["statistics", "usage", "percent"]
            ):
                summary["numerical_data"].append({"value": text, "context": attributes})
            elif any(
                keyword in extraction_class.lower()
                for keyword in ["library", "framework", "technology"]
            ):
                summary["technical_terms"].append(
                    {"term": text, "attributes": attributes}
                )
            else:
                summary["key_findings"].append(
                    {
                        "type": extraction_class,
                        "content": text,
                        "attributes": attributes,
                    }
                )

        return summary

    def _analyze_source_documents(
        self, source_docs: List[Document], query: str
    ) -> List[Dict]:
        """ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã®è©³ç´°åˆ†æ"""
        detailed_sources = []

        for i, doc in enumerate(source_docs):
            source_info = {
                "document_id": i + 1,
                "source_metadata": doc.metadata,
                "content_preview": doc.page_content[:200] + "..."
                if len(doc.page_content) > 200
                else doc.page_content,
                "relevance_score": self._calculate_relevance_score(
                    doc.page_content, query
                ),
                "exact_quotes": self._find_exact_quotes(doc.page_content, query),
            }

            detailed_sources.append(source_info)

        return detailed_sources

    def _calculate_relevance_score(self, text: str, query: str) -> float:
        """é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        query_words = set(query.lower().split())
        text_words = set(text.lower().split())

        # å…±é€šå˜èªã®å‰²åˆã‚’è¨ˆç®—
        if not query_words:
            return 0.0

        common_words = query_words.intersection(text_words)
        return len(common_words) / len(query_words)

    def _find_exact_quotes(self, text: str, query: str) -> List[str]:
        """è³ªå•ã«é–¢é€£ã™ã‚‹æ­£ç¢ºãªå¼•ç”¨ã‚’æŠ½å‡º"""
        sentences = re.split(r"[.ã€‚!ï¼?ï¼Ÿ]", text)
        query_words = set(query.lower().split())

        relevant_quotes = []
        for sentence in sentences:
            sentence_words = set(sentence.lower().split())
            # è³ªå•ã®å˜èªãŒå«ã¾ã‚Œã¦ã„ã‚‹æ–‡ã‚’æŠ½å‡º
            if query_words.intersection(sentence_words):
                relevant_quotes.append(sentence.strip())

        return relevant_quotes[:3]  # ä¸Šä½3ã¤ã®å¼•ç”¨

    def _create_verification_data(
        self, source_docs: List[Document], answer: str
    ) -> Dict:
        """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        return {
            "source_count": len(source_docs),
            "answer_length": len(answer),
            "cross_references": self._find_cross_references(source_docs),
            "consistency_check": self._check_consistency(source_docs, answer),
        }

    def _find_cross_references(self, source_docs: List[Document]) -> List[str]:
        """ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹"""
        cross_refs = []
        for doc in source_docs:
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®å‚ç…§ã‚’æ¤œç´¢
            if "å‚ç…§" in doc.page_content or "é–¢é€£" in doc.page_content:
                cross_refs.append(f"Section {doc.metadata.get('section', 'unknown')}")
        return cross_refs

    def _check_consistency(self, source_docs: List[Document], answer: str) -> str:
        """ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        # ç°¡å˜ãªä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        if len(source_docs) > 1:
            return "è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’çµ±åˆ"
        else:
            return "å˜ä¸€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®æƒ…å ±"

    def _analyze_confidence(self, source_docs: List[Document], answer: str) -> Dict:
        """ä¿¡é ¼åº¦ã‚’åˆ†æ"""
        return {
            "confidence_level": "é«˜"
            if len(source_docs) >= 3
            else "ä¸­"
            if len(source_docs) >= 2
            else "ä½",
            "reasoning": f"{len(source_docs)}å€‹ã®ã‚½ãƒ¼ã‚¹æ–‡æ›¸ã‹ã‚‰ç”Ÿæˆ",
            "verification_needed": len(source_docs) < 2,
        }

    def demo_enhanced_rag(self):
        """æ‹¡å¼µRAGã®ãƒ‡ãƒ¢"""
        print("\n=== LangExtractçµ±åˆæ‹¡å¼µRAGãƒ‡ãƒ¢ ===")

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿
        print("1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ä¸­...")
        documents = self.load_documents()
        print(f"èª­ã¿è¾¼ã‚“ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")

        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
        print("2. æ‹¡å¼µãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆä¸­...")
        self.create_vector_store(documents)

        # QAãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
        print("3. æ‹¡å¼µQAãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆä¸­...")
        self.create_qa_chain()

        # ã‚µãƒ³ãƒ—ãƒ«è³ªå•
        sample_queries = [
            "Pythonã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã¤ã„ã¦å…·ä½“çš„ãªæƒ…å ±ã‚’æ•™ãˆã¦",
        ]

        print("\nã‚µãƒ³ãƒ—ãƒ«è³ªå•ã§ã®æ‹¡å¼µRAGå®Ÿè¡Œ:")
        for i, query in enumerate(sample_queries, 1):
            print(f"\n--- ã‚µãƒ³ãƒ—ãƒ«è³ªå• {i} ---")
            print(f"è³ªå•: {query}")

            try:
                result = self.extract_detailed_data(query)
                self._display_enhanced_result(result)
            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        print("\n\n=== ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ ===")
        print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¨å…¥åŠ›ï¼‰:")

        while True:
            question = input("\nè³ªå•: ")
            if question.lower() == "quit":
                break

            try:
                result = self.extract_detailed_data(question)
                self._display_enhanced_result(result)
            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    def _display_enhanced_result(self, result: Dict):
        """LangExtractçµ±åˆã®æ‹¡å¼µçµæœã‚’è¡¨ç¤º"""
        print("\n" + "=" * 80)
        print("ã€LangExtractçµ±åˆæ‹¡å¼µRAGçµæœã€‘")
        print("=" * 80)

        print("\nã€å›ç­”ã€‘")
        print(result["answer"])

        # LangExtractåˆ†æçµæœ
        if "langextract_analysis" in result:
            langextract = result["langextract_analysis"]
            print("\nã€LangExtractæ§‹é€ åŒ–æŠ½å‡ºçµæœã€‘")
            print(
                f"æŠ½å‡ºãƒ¢ãƒ‡ãƒ«: {langextract['extraction_metadata'].get('model_used', 'N/A')}"
            )
            print(
                f"æŠ½å‡ºãƒ‘ã‚¹æ•°: {langextract['extraction_metadata'].get('extraction_passes', 'N/A')}"
            )
            print(
                f"ç·æŠ½å‡ºæ•°: {langextract['extraction_metadata'].get('total_extractions', 0)}ä»¶"
            )

            for i, extraction in enumerate(langextract.get("extractions", []), 1):
                print(f"\n  ğŸ“Š æŠ½å‡ºé …ç›® {i}:")
                print(f"     åˆ†é¡: {extraction.get('class', 'N/A')}")
                print(f'     æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ: "{extraction.get("text", "N/A")}"')

                if extraction.get("attributes"):
                    print("     å±æ€§:")
                    for key, value in extraction["attributes"].items():
                        print(f"       â€¢ {key}: {value}")

                if extraction.get("source_position", {}).get("start") is not None:
                    pos = extraction["source_position"]
                    print(
                        f"     ä½ç½®: æ–‡å­—{pos.get('start', 'N/A')}-{pos.get('end', 'N/A')}"
                    )

                if extraction.get("confidence"):
                    print(f"     ä¿¡é ¼åº¦: {extraction['confidence']:.2f}")

        print("\nã€è©³ç´°ã‚½ãƒ¼ã‚¹æƒ…å ±ã€‘")
        for source in result["detailed_sources"]:
            print(f"\n  ğŸ“„ æ–‡æ›¸ {source['document_id']}:")
            print(f"     ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {source['source_metadata'].get('section', 'N/A')}")
            print(f"     é–¢é€£æ€§ã‚¹ã‚³ã‚¢: {source['relevance_score']:.2f}")
            print(f"     å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {source['content_preview']}")

            # å„ã‚½ãƒ¼ã‚¹ã®LangExtractçµæœ
            if (
                "langextract_data" in source
                and source["langextract_data"]["extractions"]
            ):
                print(
                    f"     LangExtractæŠ½å‡ºæ•°: {len(source['langextract_data']['extractions'])}ä»¶"
                )
                for ext in source["langextract_data"]["extractions"][:2]:  # ä¸Šä½2ã¤
                    print(
                        f'       â€¢ {ext.get("class", "N/A")}: "{ext.get("text", "N/A")}"'
                    )

            if source["exact_quotes"]:
                print("     æ­£ç¢ºãªå¼•ç”¨:")
                for quote in source["exact_quotes"][:1]:  # æœ€åˆã®1ã¤
                    print(f'       â€¢ "{quote[:80]}..."')

        print("\nã€æ¤œè¨¼æƒ…å ±ã€‘")
        verification = result["verification_data"]
        print(f"  â€¢ ã‚½ãƒ¼ã‚¹æ–‡æ›¸æ•°: {verification['source_count']}")
        print(f"  â€¢ ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯: {verification['consistency_check']}")
        if verification["cross_references"]:
            print(
                f"  â€¢ ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹: {', '.join(verification['cross_references'])}"
            )

        print("\nã€ä¿¡é ¼åº¦åˆ†æã€‘")
        confidence = result["confidence_analysis"]
        print(f"  â€¢ ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«: {confidence['confidence_level']}")
        print(f"  â€¢ æ ¹æ‹ : {confidence['reasoning']}")
        if confidence["verification_needed"]:
            print("  â€¢ âš ï¸  è¿½åŠ æ¤œè¨¼ãŒæ¨å¥¨ã•ã‚Œã¾ã™")

        print("=" * 80)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    load_dotenv()

    print("LangExtractçµ±åˆæ‹¡å¼µRAGã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    print("é€šå¸¸ã®RAGã‚’è¶…ãˆãŸè©³ç´°ãªã‚½ãƒ¼ã‚¹è¿½è·¡ã¨æ¤œè¨¼æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™")

    try:
        enhanced_rag = LangExtractRAG()
        enhanced_rag.demo_enhanced_rag()

        print("\n=== æ‹¡å¼µRAGãƒ‡ãƒ¢ãŒå®Œäº†ã—ã¾ã—ãŸ ===")
        print("ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®è¿½åŠ æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™:")
        print("â€¢ è©³ç´°ãªã‚½ãƒ¼ã‚¹ä½ç½®æƒ…å ±ï¼ˆè¡Œç•ªå·ã€æ–‡å­—ä½ç½®ï¼‰")
        print("â€¢ æ•°å€¤ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®æ­£ç¢ºãªæŠ½å‡º")
        print("â€¢ ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã¨æ¤œè¨¼æƒ…å ±")
        print("â€¢ ä¿¡é ¼åº¦åˆ†æã¨ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯")
        print("â€¢ è¨ˆç®—éç¨‹ã®é€æ˜æ€§")

    except KeyboardInterrupt:
        print("\nå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
